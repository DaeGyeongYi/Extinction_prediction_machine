{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mlpckg.util_method import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_key() argument (\"1.novem/ 2.trans/ 3.bipun/ 4.paren \")\n",
    "# select_key() return (target_species,key)\n",
    "\n",
    "num = 4\n",
    "target_species,key= select_key(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your path for AOO Crawling\n",
    "smooth_path = \"C:/Users/DAEGYEONGLEE/Desktop/ladybug_project/Data/4th_tmp/forAOO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_var = pd.read_csv(\"./Data/3rd_manipulated_data/shap_values/\"+target_species+\"shap_values_top15.csv\")\n",
    "shap_var = list(shap_var.name.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./Data/2nd_manipulated_data/20220808_\" + str(key)+\"_train_minmax.csv\")\n",
    "df_train = df_train.drop(df_train[(df_train[target_species + str('_18')]!=0)&(df_train['Species']!= target_species)].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./Data/2nd_manipulated_data/20220808_\" + str(key)+\"_test_minmax.csv\")\n",
    "df_test = df_test[df_test.Species==target_species].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain columns which you need\n",
    "delivered_package = set(df_train.columns.tolist()) - set(['Source','State','Year','Latitude','Longitude','public_positional_accuracy', \n",
    "                                                    str(target_species+str('_18'))])\n",
    "\n",
    "delivered_package = list(delivered_package)\n",
    "\n",
    "df_train = df_train.sort_values(by='Species').reset_index(drop=True)\n",
    "df_train_18 = df_train[delivered_package]\n",
    "\n",
    "\n",
    "df_train_18['target'] = 0\n",
    "df_train_18.loc[df_train.Species==target_species,'target']=1\n",
    "df_train_18 = df_train_18.sort_values(by='target', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_18.rename(columns={\"Species\":\"Species_18\"},inplace=True)\n",
    "df_train_18.columns = [i.replace(\" \",\"_\").replace(\"-\",\"\") for i in df_train_18.columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "def AOO(df_train,df_test,targetnum,varlist,target_species,Year_start,Year_end,\n",
    "bagstart=0,bagnum=50,repstart=0,repnum=50,learningrate=0.7,judge_indicator=0.5,to_csv=False):\n",
    "    hot = list(range (bagstart, bagnum))\n",
    "    pot = list(range(repstart, repnum))\n",
    "    Year_end += 1\n",
    "    for i in range(Year_start,Year_end):\n",
    "            globals()['potcast_{}'.format(i)] = []\n",
    "            \n",
    "            \n",
    "    for k in tqdm(hot):\n",
    "        BOX = df_train.reset_index(drop = True).copy()\n",
    "        absence_pool = BOX[targetnum:].sample(n=targetnum, random_state = k)\n",
    "        Ml_pool = pd.concat([BOX[:targetnum], absence_pool])\n",
    "        answer = Ml_pool.target\n",
    "        Ml_pool = Ml_pool[varlist]\n",
    "\n",
    "\n",
    "\n",
    "        for i in pot:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Ml_pool, answer, test_size=0.000000000001, random_state= i)  \n",
    "            xgb_wrapper = XGBClassifier(n_estimators=1000, learning_rate=learningrate, max_depth=7, objective = \"binary:logistic\")\n",
    "            evals = [(X_test, y_test)]\n",
    "            xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                                eval_metric=\"error\", eval_set=evals, verbose=0)\n",
    "\n",
    "            \n",
    "            for year in range(Year_start,Year_end):\n",
    "                nameind={}\n",
    "                \n",
    "                # Alert!\n",
    "                # test csv를 만들때 실패했던 종들은 따로 코딩해줘야함.\n",
    "                # When making test csv file, the failed species should be coded separately.\n",
    "                for name in varlist: \n",
    "                    if name =='Coelophora_maculata_18':\n",
    "                        nameind['phoramaculata_'+str(year)+\"_18km\"] = name\n",
    "\n",
    "                    elif name =='Epilachna_borealis_18':\n",
    "                        nameind['achnaborealis_'+str(year)+\"_18km\"] = name\n",
    "\n",
    "                    elif name =='Coleomegilla_maculata_18':\n",
    "                        nameind['gillamaculata_'+str(year)+\"_18km\"] = name\n",
    "\n",
    "                    elif name =='Psyllobora_borealis_18':\n",
    "                        nameind['boraborealis_'+str(year)+\"_18km\"] = name\n",
    "\n",
    "                    elif name =='Olla_vnigrum_18':\n",
    "                        nameind['v-nigrum_'+str(year)+\"_18km\"] = name\n",
    "                    else:\n",
    "                        nameind[(name.split(\"_\")[1]+\"_\"+str(year)+\"_18km\")] = name\n",
    "\n",
    "                df09 = df_test[list(nameind.keys())].reset_index(drop=True)\n",
    "\n",
    "\n",
    "                df09.columns = list(nameind.values())\n",
    "                preds_proba = xgb_wrapper.predict_proba(df09)\n",
    "                presence_index = [p for p in range(len(preds_proba)) if preds_proba[p][1]>0.5]\n",
    "\n",
    "                \n",
    "                globals()[\"potcast_{}\".format(year)].extend(presence_index)\n",
    "\n",
    "    judgebythis = len(hot)*len(pot)*judge_indicator\n",
    "    \n",
    "\n",
    "    for year in range(Year_start,Year_end):\n",
    "        globals()['count{}'.format(year)] = Counter(globals()['potcast_{}'.format(year)])          \n",
    "        globals()['idx_{}'.format(year)] = []\n",
    "        for i in globals()['count{}'.format(year)].most_common(targetnum):\n",
    "            if i[1]>=judgebythis:\n",
    "                globals()['idx_{}'.format(year)].append(i[0])\n",
    "\n",
    "    df_test['occurrenceRemarks'] = 'Note'\n",
    "    df_test.rename(columns={\"Latitude\":\"latitude\",\n",
    "                   \"Longitude\":\"longitude\"},inplace=True)\n",
    "\n",
    "   \n",
    "\n",
    "    for i in range(Year_start,Year_end):\n",
    "        globals()['df_{}'.format(i)]=df_test.iloc[globals()['idx_{}'.format(i)]][['latitude','longitude','Species','occurrenceRemarks','Year']].rename({'Species':'scientificname'},axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_modi = df_test[['latitude','longitude','Species','occurrenceRemarks','Year']].rename({'Species':'scientificname'},axis=1)\n",
    "\n",
    "    # 머신은 없다고 판단했으나, 실제로는 있는 경우와 합침 (aoo니까)\n",
    "    # Even in areas where machine learning predictions indicate that there is no target species, there may be target species in the actual identification record. \n",
    "    # In order to estimate AOO, we need to fix this case.\n",
    "\n",
    "    for i in range(Year_start,Year_end):\n",
    "        globals()['df_fin{}'.format(i)] = pd.concat([globals()['df_{}'.format(i)],df_modi[df_modi.Year==i]]).drop_duplicates(keep='first').reset_index(drop=True)\n",
    "        print(i,\"\",len(globals()['df_fin{}'.format(i)]))\n",
    "    if to_csv == True:\n",
    "        for i in range(Year_start,Year_end):\n",
    "            globals()['df_fin{}'.format(i)].to_csv(\"./Data/4th_tmp/forAOO/\"+target_species+\"_aoo\"+str(i)+\".csv\", index = None)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AOO 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import math\n",
    "\n",
    "AOO_pot_mix = []\n",
    "Year_pot_mix = []\n",
    "std_pot_mix = []\n",
    "var_pot_mix = []\n",
    "mean_pot_mix = []\n",
    "AOO_nav = pd.DataFrame({\"Year\":[],\"AOO\":[]})\n",
    "\n",
    "for samp in tqdm(range(0,30)):\n",
    "\n",
    "    AOO(df_train = df_train_18,\n",
    "    df_test = df_test ,\n",
    "    targetnum=len(df_test),\n",
    "    varlist=shap_var,Year_start=2007,Year_end=2021,\n",
    "    target_species=target_species,bagstart=(samp)*50,bagnum=(samp+1)*50\n",
    "    ,repstart=(samp)*50,repnum=(samp+1)*50,learningrate=0.7,to_csv=True)\n",
    "    \n",
    "    \n",
    "    nav = {}\n",
    "    ex = []\n",
    "    for i in range(2007,2022):\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get('http://geocat.kew.org/')\n",
    "            driver.find_element(By.XPATH,'/html/body/section[2]/div/header/a').click()\n",
    "            time.sleep(0.5)\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"welcome\"]/div[2]/ul/li[2]/a').click()\n",
    "            time.sleep(0.5)\n",
    "            driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div/div[3]/div[2]/div/div/div/div/ul/li[6]/a').click()\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "            # send_keys argument:absolute path of AOO csv\n",
    "            driver.find_element(By.CSS_SELECTOR,\"#add_source_container > div > div > div > ul > li.upload_source.selected > div > div.idle > div > form > input[type=file]\").send_keys(smooth_path + target_species+\"_aoo\"+str(i)+\".csv\")\n",
    "\n",
    "\n",
    "            time.sleep(12)\n",
    "            # upload your file\n",
    "            driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div/div[3]/div[2]/div/div/div/div/ul/li[6]/div/div[3]/a[2]').send_keys(Keys.ENTER)\n",
    "            time.sleep(15)\n",
    "            #check aoo text\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"toggle_analysis\"]/span').click()\n",
    "            time.sleep(0.5)\n",
    "            # save aoo text\n",
    "            nav[str(i)]=int(driver.find_element(By.CSS_SELECTOR,'#tools > div > div.center > div.center-tool > div.right > div.analysis > div > div.analysis_data > ul > li:nth-child(2) > p:nth-child(2)').text.replace(\",\",\"\").split(\".\")[0])\n",
    "            # quit\n",
    "            driver.quit()\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            ex.append(i)\n",
    "            driver.quit()\n",
    "            print(i,\"Error\", e)\n",
    "            pass\n",
    "    \n",
    "\n",
    "    for i in ex:\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get('http://geocat.kew.org/')\n",
    "            driver.find_element(By.XPATH,'/html/body/section[2]/div/header/a').click()\n",
    "            time.sleep(0.5)\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"welcome\"]/div[2]/ul/li[2]/a').click()\n",
    "            time.sleep(0.5)\n",
    "            driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div/div[3]/div[2]/div/div/div/div/ul/li[6]/a').click()\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "            # send_keys 뒤에 절대경로 넣기\n",
    "            driver.find_element(By.CSS_SELECTOR,\"#add_source_container > div > div > div > ul > li.upload_source.selected > div > div.idle > div > form > input[type=file]\").send_keys(smooth_path + target_species+\"_aoo\"+str(i)+\".csv\")\n",
    "\n",
    "\n",
    "            time.sleep(15)\n",
    "            #업로드시키기\n",
    "            driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div/div[3]/div[2]/div/div/div/div/ul/li[6]/div/div[3]/a[2]').send_keys(Keys.ENTER)\n",
    "            time.sleep(20)\n",
    "            #aoo 텍스트 따오기\n",
    "            driver.find_element(By.XPATH,'//*[@id=\"toggle_analysis\"]/span').click()\n",
    "            time.sleep(0.5)\n",
    "            #aoo 텍스트 저장\n",
    "            nav[str(i)]=int(driver.find_element(By.CSS_SELECTOR,'#tools > div > div.center > div.center-tool > div.right > div.analysis > div > div.analysis_data > ul > li:nth-child(2) > p:nth-child(2)').text.replace(\",\",\"\").split(\".\")[0])\n",
    "            #창닫기\n",
    "            driver.quit()\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            driver.quit()\n",
    "            print(i,\"Error Final\", e)\n",
    "            pass\n",
    "\n",
    "    nav = dict(sorted(nav.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "\n",
    "    AOO_fin = pd.DataFrame({\"Year\":nav.keys(),\"AOO\":nav.values()})\n",
    "    AOO_fin['Year'] = AOO_fin['Year'].astype(float)\n",
    "    AOO_fin['AOO'] = AOO_fin['AOO'].astype(float)\n",
    "\n",
    "    AOO_nav = pd.concat([AOO_nav,AOO_fin])\n",
    "\n",
    "\n",
    "    ############# reduction rate\n",
    "    Reduction_rate = pd.DataFrame({\"Start_Year\":[],\"AOO\":[]})\n",
    "    Reduction_rate['Start_Year'] = list(range(2007,2013))\n",
    "\n",
    "    try:\n",
    "        Reduction_rate['AOO'][0] = (float(AOO_fin['AOO'][0]) - float(AOO_fin['AOO'][9]))/float(AOO_fin['AOO'][0])\n",
    "        Reduction_rate['AOO'][1] = (float(AOO_fin['AOO'][1]) - float(AOO_fin['AOO'][10]))/float(AOO_fin['AOO'][1])\n",
    "        Reduction_rate['AOO'][2] = (float(AOO_fin['AOO'][2]) - float(AOO_fin['AOO'][11]))/float(AOO_fin['AOO'][2])\n",
    "        Reduction_rate['AOO'][3] = (float(AOO_fin['AOO'][3]) - float(AOO_fin['AOO'][12]))/float(AOO_fin['AOO'][3])\n",
    "        Reduction_rate['AOO'][4] = (float(AOO_fin['AOO'][4]) - float(AOO_fin['AOO'][13]))/float(AOO_fin['AOO'][4])\n",
    "        Reduction_rate['AOO'][5] = (float(AOO_fin['AOO'][5]) - float(AOO_fin['AOO'][14]))/float(AOO_fin['AOO'][5])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    AOO_pot_mix.append(Reduction_rate['AOO'].max())\n",
    "    \n",
    "    try:\n",
    "        Year_pot_mix.append(int(Reduction_rate[Reduction_rate['AOO'] == Reduction_rate['AOO'].max()]['Start_Year'].values))\n",
    "    except:\n",
    "        Year_pot_mix.append(0)\n",
    "\n",
    "    std_pot_mix.append(Reduction_rate['AOO'].std())\n",
    "    var_pot_mix.append(Reduction_rate['AOO'].var())\n",
    "    mean_pot_mix.append(Reduction_rate['AOO'].mean())\n",
    "\n",
    "\n",
    "AOO_pot_mix = [0 if math.isnan(x) else x for x in AOO_pot_mix]\n",
    "Year_pot_mix = [0 if math.isnan(x) else x for x in Year_pot_mix]\n",
    "std_pot_mix = [0 if math.isnan(x) else x for x in std_pot_mix]\n",
    "var_pot_mix = [0 if math.isnan(x) else x for x in var_pot_mix]\n",
    "mean_pot_mix = [0 if math.isnan(x) else x for x in mean_pot_mix]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end = pd.DataFrame({\"AOO_pot_mix\":AOO_pot_mix,\n",
    "            \"mean_pot_mix\":mean_pot_mix,\n",
    "            \"std_pot_mix\":std_pot_mix,\n",
    "            'var_pot_mix':var_pot_mix,\n",
    "            'Year_pot_mix':Year_pot_mix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end.to_csv(\"./Data/4th_tmp/\"+key+\"_interval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOO_nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOO_nav.to_csv(\"./Data/4th_tmp/\"+key+\"_AOO_check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31e7f64e5f794f3d9e14a1c6aa6032c55619d5e8f9c2735175e834e99d666003"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
